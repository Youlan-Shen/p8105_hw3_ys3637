---
title: "p8105_hw3_ys3637"
author: "Youlan Shen"
date: "2022-10-15"
output: github_document
---

```{r}
# library all packages that we need at the beginning
library(tidyverse)
library(ggridges)

# default set up
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets using:

```{r}
library(p8105.datasets)
data("instacart")
```

## Problem 2

Accelerometers have become an appealing alternative to self-report techniques for studying physical activity in observational studies and clinical trials, largely because of their relative objectivity. During observation periods, the devices measure “activity counts” in a short period; one-minute intervals are common. Because accelerometers can be worn comfortably and unobtrusively, they produce around-the-clock observations.

This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The data can be downloaded here. In this spreadsheet, variables activity.* are the activity counts for each minute of a 24-hour day starting at midnight.

### Read in, tidy, and describe the data

Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

```{r}
# read in data from CSV file
accel_data <- read_csv("data/accel_data.csv")
# show the first several lines of the original data
head(accel_data)
# clean, tidy, wrangle the data
accel_data <- accel_data %>% 
  janitor::clean_names() %>% 
  rename(day_type = day) %>% 
  pivot_longer(
    c(activity_1:activity_1440),
    names_to = "minute", 
    values_to = "activity_counts",
    names_prefix = 'activity_') %>% 
  mutate(
    weekday_vs_weekend = ifelse((day_type == "Saturday" | day_type == "Sunday"), "weekend", "weekday"),
    activity_counts = as.double(activity_counts),
    minute = as.integer(minute))
# show the cleaned data
accel_data
```

This dataset contains `r nrow(accel_data)` rows and `r ncol(accel_data)` columns, while each row showing an activity measure in a single minute during the 5-week data collection period. Variables include week (from the first week to the fifth week), day_id(from the first day to the 35th day), day_type (includes Monday to Sunday), minute (every minute in a single day), activity_counts (the count that is recorded in the accelerometer), and weekday_vs_weekend (which distinguishs weekdays and weekend). There are total `r nrow(accel_data)` observations, which are the activity measures from every minute in a day during the 5-week period.

### Traditional analyses on total activity counts over a day

The following table shows the total activity counts for each day in this study period in descending order. In general, we can see from the table that there is a huge difference between different days, the highest counts could be 685929 while the lowest is 1440, so the range is very huge. By including weekday vs weekend variable, we could see that usually the person has high activity counts on weekdays. But there is not much difference over different weeks.

```{r}
accel_data %>% 
  group_by(day_id, weekday_vs_weekend, week) %>% 
  summarize(total_activity = sum(activity_counts)) %>% 
  arrange(desc(total_activity)) %>% 
  knitr::kable()
```

### Draw a graph of activity

```{r}
# produce the week 1 graph
graph_produce <- function(x, a){
  x %>%
  filter(week == a) %>% 
  mutate(day_type = forcats::fct_relevel(day_type, c("Monday", "Tuesday",
        "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  ggplot(aes(x = minute, y = activity_counts, color = day_type)) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = paste("Activity Counts Over A Day in Week ", as.character(a)),
    x = "Each Minute",
    y = "Activity Counts"
  )
}
graph_produce(accel_data, 1)
```

```{r}
# produce the week 2 graph
graph_produce(accel_data, 2)
```

```{r}
# produce the week 3 graph
graph_produce(accel_data, 3)
```

```{r}
# produce the week 4 graph
graph_produce(accel_data, 4)
```

```{r}
# produce the week 5 graph
graph_produce(accel_data, 5)
```

```{r}
# produce the total period graph
accel_data %>%
  mutate(day_type = forcats::fct_relevel(day_type, c("Monday", "Tuesday",
        "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  ggplot(aes(x = minute, y = activity_counts, color = day_type)) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Activity Counts Over A Day in Total 5 Weeks",
    x = "Each Minute",
    y = "Activity Counts",
  )
```

From the graph, the person tends to have higher activity counts over the weekdays, especially Friday, Thursday, and Wednesday, and over the weekend, the person tends to have lower activity counts. These 5 weeks are kind of different from each other, but in general, the person has a similar pattern--Friday usually has the highest activity counts, and Saturday usually has the lowest activity counts, weekday activity counts are higher than the weekend.

## Problem 3

```{r}
library(p8105.datasets)
data("ny_noaa")
```

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. 

#### Read in the data

```{r}
data("ny_noaa")

ny_noaa = 
  ny_noaa %>% 
  as_tibble(ny_noaa)

# count the rows of the data
n <- nrow(ny_noaa)
# show the data
ny_noaa
# count the NAs
ny_noaa %>% 
  summarise(na_prcp = sum(is.na(prcp)) / n,
            na_snow = sum(is.na(snow)) / n,
            na_snwd = sum(is.na(snwd)) / n,
            na_tmax = sum(is.na(tmax)) / n,
            na_tmin = sum(is.na(tmin)) / n)
```

This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns, while each row showing the weather measures in a specific date recorded in weather station. Variables include id (the weather station ID), date (the date that weather data observed), prcp (precipitaion on that day), snow (snowfall in mm), snwd (snow depth in mm), tmax (maximum temperature in C), and tmin (minimum temperature in C). There are total `r nrow(ny_noaa)` observations, which are the weather data recorded in a time period. 

From the above Count The NAs result, the percentage of NAs in prcp is 0.0562, which is not a big issue. The percentage of NAs in snow is 0.147, in snwd is 0.228, in tmax is 0.437, in tmin is 0.437. I think that the percentage of NAs in a data over 0.05 could be considered as an issue, so actually snow, snwd, tmax, and tmin have issues with missing values, especially tmax and tmin--lost almost half of the data.

#### Clean the data

```{r}
# separate variables for year, month, and day
ny_noaa <- ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-")
# Ensure observations for temperature, precipitation, and snowfall are given in reasonable units
ny_noaa <- ny_noaa %>% 
  mutate(prcp = as.double(prcp), tmax = as.double(tmax), 
         tmin = as.double(tmin), snow = as.double(snow))
ny_noaa %>% 
  arrange(desc(snow)) %>% 
  head()
# show the most commonly observed values in snow
ny_noaa %>% 
  select(snow) %>% 
  group_by(snow) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

So the most commonly observed values in snow is 0, NA, 25, 13, 51, 76, 8, 5, 38, 3 in mm.

### Make a two-panel plot showing the average max temperature in January and in July

```{r}
ny_noaa %>% 
  select(id, month, tmax) %>% 
  filter(month == "01" | month == "07") %>% 
  group_by(id, month) %>% 
  summarise(avg_tmax = mean(tmax))
```

### Make a two-panel plot showing (i) tmax vs tmin for the full dataset

```{r}

```


### Make a two-panel plot showing (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.



